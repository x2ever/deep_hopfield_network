{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "latent_size = 16\n",
    "label_size = 10\n",
    "\n",
    "\n",
    "class MaxOneHot(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        idx = torch.argmax(input)\n",
    "        ctx._input_shape = input.shape\n",
    "        ctx._input_dtype = input.dtype\n",
    "        ctx._input_device = input.device\n",
    "        ctx.save_for_backward(idx)\n",
    "        output = torch.zeros(ctx._input_shape, device=ctx._input_device, dtype=ctx._input_dtype)\n",
    "        output[idx] = input[idx]\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        idx, = ctx.saved_tensors\n",
    "        grad_input = torch.zeros(ctx._input_shape, device=ctx._input_device, dtype=ctx._input_dtype)\n",
    "        grad_input[idx] = grad_output[idx]\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "class Softmax(torch.nn.Module):\n",
    "    def __init__(self, latent_size, label_size):\n",
    "        super(Softmax, self).__init__()\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(latent_size, label_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.softmax(self.fc1(x), dim=1)\n",
    "\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, latent_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.latent_size = latent_size\n",
    "\n",
    "        self.conv1 = torch.nn.Conv2d(1, 32, 5, padding=2)\n",
    "        self.conv2 = torch.nn.Conv2d(32, 64, 5, padding=2)\n",
    "        self.fc1 = torch.nn.Linear(64 * 28 * 28, latent_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.elu(self.conv1(x))\n",
    "        x = F.elu(self.conv2(x))\n",
    "        x = x.view(-1, 64 * 28 * 28)   # reshape Variable\n",
    "        x = self.fc1(x)\n",
    "        return torch.tanh(x)\n",
    "\n",
    "\n",
    "class Hopfield(torch.nn.Module):\n",
    "    def __init__(self, latent_size, label_size):\n",
    "        super(Hopfield, self).__init__()\n",
    "        self.latent_size = latent_size\n",
    "        self.label_size = label_size\n",
    "        label_latent_vector = 2 * torch.rand((label_size, latent_size), device=device) - 1\n",
    "        self.label_latent_vectors = label_latent_vector.clone().requires_grad_(True)\n",
    "        self.max_one_hot = MaxOneHot.apply\n",
    "\n",
    "    def forward(self, s):\n",
    "        weight = self._get_weight()\n",
    "\n",
    "        min_e = self._get_energy(s, weight)\n",
    "        min_s = s\n",
    "\n",
    "        for _ in range(self.latent_size):\n",
    "            prev_s = s.clone()\n",
    "            s = torch.sign(weight @ prev_s) * torch.abs(prev_s)\n",
    "            e = self._get_energy(s, weight)\n",
    "            if min_e > e:\n",
    "                min_e = e\n",
    "                min_s = s\n",
    "\n",
    "        return self.max_one_hot(torch.abs(min_s @ self.label_latent_vectors.T)) / self.latent_size\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_energy(s, w):\n",
    "        return - s @ w @ s\n",
    "\n",
    "    def _get_weight(self):\n",
    "\n",
    "        x = self.label_latent_vectors\n",
    "        rho = torch.mean(x)\n",
    "\n",
    "        for i, x_ in enumerate(x):\n",
    "            temp = x_ - rho\n",
    "            if i == 0:\n",
    "                weight = torch.ger(temp, temp)\n",
    "            else:\n",
    "                weight += torch.ger(temp, temp)\n",
    "\n",
    "        diag_weight = torch.diag(torch.diag(weight))\n",
    "        weight = weight - diag_weight\n",
    "        weight /= len(x)\n",
    "\n",
    "        return weight\n",
    "\n",
    "\n",
    "class DeepHopfield(torch.nn.Module):\n",
    "    def __init__(self, latent_size, label_size):\n",
    "        super(DeepHopfield, self).__init__()\n",
    "        self.encoder = Encoder(latent_size).to(device)\n",
    "        self.softmax = Softmax(latent_size, label_size).to(device)\n",
    "        self.hopfield = Hopfield(latent_size, label_size)\n",
    "        self.label_size = label_size\n",
    "        self.latent_size = latent_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent_vectors = self.encoder(x)\n",
    "\n",
    "        hopfield_labels = torch.stack([\n",
    "            self.hopfield(latent_vector)\n",
    "            for latent_vector in latent_vectors\n",
    "        ])\n",
    "\n",
    "        return hopfield_labels, self.softmax(latent_vectors)\n",
    "\n",
    "    def optimizer(self):\n",
    "        opt = \\\n",
    "            torch.optim.Adam([\n",
    "                {'params': self.encoder.parameters()},\n",
    "                {'params': self.softmax.parameters()},\n",
    "                {'params': self.hopfield.label_latent_vectors}\n",
    "            ], lr=0.01)\n",
    "        return opt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    from torchvision import datasets, transforms\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('.', train=True, download=True, transform=transforms.ToTensor()), batch_size=512, shuffle=True\n",
    "    )\n",
    "\n",
    "    model = DeepHopfield(16, 10)\n",
    "    optimizer = model.optimizer()\n",
    "\n",
    "    cross_entropy_loss_fn = torch.nn.CrossEntropyLoss().to(device)\n",
    "    for step in range(10000):\n",
    "        for b, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            label0, label1 = model(data)\n",
    "\n",
    "            cross_entropy_loss0 = cross_entropy_loss_fn(label0, target)\n",
    "            # cross_entropy_loss1 = cross_entropy_loss_fn(label1, target)\n",
    "            loss = cross_entropy_loss0  # + cross_entropy_loss1\n",
    "\n",
    "            loss.backward()\n",
    "            print(f\"[Epoch: {step: 05d} | {b: 02d} / {len(train_loader) - 1: 02d}] \"\n",
    "                  f\"loss: {loss.data: .5f} grad: {torch.sum(torch.abs(model.hopfield.label_latent_vectors.grad))}\", end='\\r')\n",
    "            optimizer.step()\n",
    "\n",
    "        print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
